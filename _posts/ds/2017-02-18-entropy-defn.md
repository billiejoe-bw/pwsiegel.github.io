---
layout: post
permalink: /entropy-defn/
title: On the Definition of Shannon Entropy
abstract: The definition of Shannon entropy admits a variety of appealing characterizations; here I will explore the characterization via "average surprisal".
date: 2017-02-18
categories: stage
---

In a [previous post]({{ site.baseurl }}{% post_url ds/2017-02-12-info-theory-basics %}) I went through some of the key ideas in Shannon's landmark paper *A Mathematical Theory of Communication*.  Perhaps the most enduring innovation in that paper is Shannon's definition of the *entropy* of a random variable which Shannon used as a measure of how much information the random variable produces.

These days information theory is a cornerstone of pure and applied mathematics, and as such its foundations have been examined quite carefully.  In my post above I introduced a characterization of Shannon entropy (due, I believe, to Myron Tribus) as a measure of how "surprising" an event generated by a random variable is on average; in this post I will attempt to justify this characterization rigorously.

## Surprise!

In what follows we will use the notation $(\Omega, \Sigma, \P)$ for probability spaces, so that $(\Omega, \Sigma)$ is a measurable space equipped with a probability measure $\P$.  We will write $\R^\pm$ for the compactification of $\R$ with a point at $\infty$ and a point at $-\infty$ adjoined.

<div class="definition">
A *surprise function* on a probability space $(\Omega, \Sigma, \P)$ is a function $S \colon \Sigma \to \R^+$ with the following properties:

1. $S(E_1) = S(E_2)$ whenever $\P(E_1) = \P(E_2)$.
2. $S(E_1) < S(E_2)$ whenever $\P(E_1) > \P(E_2)$.
3. If $E_1$ and $E_2$ are independent events then $S(E_1 \cap E_2) = S(E_1) + S(E_2)$.
</div>

In plain language: the first condition says that surprisal of an event depends only on its probability; the second condition says that low probability events are more surprising than high probability events; and the third condition says that surprise is additive when two independent events occur simultaneously.

There is a standard example of a surprise function:

<div class="lemma">
Let $(\Omega, \Sigma, \P)$ be a probability space.  Then the function $S \colon \Sigma \to \R^\pm$ given by:

$$S(E) = -\log \P(E)$$

(The base of the logarithm can be any positive real number; changing the base scales $S$ by a positive constant.  We define $\log 0 = -\infty$.)
</div>
<div class="proof">
Condition 1 in the definition of surprise function is obvious from the formula for $S$.  Condition 2 follows from the fact that the logarithm function is strictly increasing.  To prove condition 3, recall that $\P(E_1 \cap E_2) = \P(E_1)\P(E_2)$ whenever $E_1$ and $E_2$ are independent (by definition), so we have:

$$
\begin{align*}
S(E_1 \cap E_2) &= -C \log \P(E_1 \cap E_2) \\
&= -C \log \P(E_1) \P(E_2) \\
&= -C \log \P(E_1) - C \log \P(E_2) \\
&= S(E_1) + S(E_2)
\end{align*}
$$

</div>

## Characterizing Surprise

It turns out that $S(E) = -\log \P(E)$ is, in a sense, the *only* possible surprise function.  There is a seductively simple proof of this fact which is unfortunately incomplete.  The argument goes as follows: the first condition in the definition of surprise asserts that $S(E) = f(\P(E))$ for some function $f \colon [0,1] \to \R^\pm$.  The second condition forces $f$ to be monotonically decreasing, and the third condition imposes the relation $f(xy) = f(x) + f(y)$.  With a bit of calculus one can prove that the only function which satisfies these conditions is $f(x) = -C \log x$.  This argument appears in [wikipedia][1], for instance.

But it is not correct as stated.  The problem is that the function $f(x)$ is only defined when $x$ has the form $\P(E)$ for some event $E$, and the relation $f(xy) = f(x) + f(y)$ only holds for numbers $x$ and $y$ of the form $x = \P(E)$ and $y = \P(F)$ for *independent* events $E$ and $F$.  What happens when there just aren't very many values in the range of $\P$ and/or there aren't very many pairs of independent events?

Here's an example.  Consider a weighted coin flip, with $\Omega = \{H, T\}$, $\Sigma = \{\emptyset, \{H\}, \{T\}, \Omega\}$, and probabilities $\P(\{H\}) = \frac{1}{4}$, $\P(\{T\}) = \frac{3}{4}$.  (Of course, $\P(\emptyset) = 0$ and $\P(\Omega) = 1$.)  There are only a few independent events in this probability space: the events $\{H\}$ and $\{T\}$ are each independent of $\emptyset$ and $\Omega$, but that's it!  These events are enough to force $S(\emptyset) = \infty$ and $S(\Omega) = 0$ (see below), but the only other constraint on $S$ is that $S(\{H\}) > $S(\{T\})$.  By contrast, if the formula $S(E) = -\log \P(E)$ were forced upon us then we would have the constraint:

$$S(\{H\}) - S(\{T\}) = -\log 4 + \log \frac{3}{4} = \log 3$$

So in order to obtain a genuine characterization of the surprise function we need to give ourselves a lot more independent events to work with.  I couldn't find any references to this issue in the literature, but I believe that I found an additional axiom which makes everything work out.

## The Trial Space

The basic idea is to insist that the surprise function extend beyond the initial probability space $(\Omega, \Sigma, \P)$ to additional probability spaces $(\Omega_n, \Sigma_n, \P_n)$ which encode repeated independent trials.  In other words, we can't measure our level of surprise by flipping a coin just once - we have to flip it many times before we can make a judgement.  This idea, by the way, was more or less built into Shannon's own characterization of entropy, so it's not so surprising (pun unintended) that it should show up here.

To express things rigorously I will introduce some language which undoubtedly already has a counterpart in the literature.  If somebody has any good references, do let me know.









[1]: https://en.wikipedia.org/wiki/Self-information "Self-information"
