---
layout: post
permalink: /entropy-defn/
title: On the Definition of Shannon Entropy
abstract: The definition of Shannon entropy admits a variety of appealing characterizations; here I will explore the characterization via "average surprisal".
date: 2017-02-18
categories: stage
---

In a [previous post]({{ site.baseurl }}{% post_url ds/2017-02-12-info-theory-basics %}) I went through some of the key ideas in Shannon's landmark paper *A Mathematical Theory of Communication*.  Perhaps the most enduring innovation in that paper is Shannon's definition of the *entropy* of a random variable which Shannon used as a measure of how much information the random variable produces.

These days information theory is a cornerstone of pure and applied mathematics, and as such its foundations have been examined quite carefully.  In my post above I introduced a characterization of Shannon entropy (due, I believe, to Myron Tribus) as a measure of how "surprising" an event generated by a random variable is on average; in this post I will attempt to justify this characterization rigorously.  Moreover I will work in the language of abstract measure theory so that I will not need to handle the discrete and continuous cases separately.

## Setup

Let $(\Omega, \Sigma, \P)$ be a probability space, meaning $(\Omega, \Sigma)$ is a measurable space equipped with a probability measure $\P$.

<div class="definition">
A *surprise function* on a probability space $(\Omega, \Sigma, \P)$ is a function $S \colon \Sigma \to \R$ with the following properties:

1. $S(E_1) = S(E_2)$ whenever $\P(E_1) = \P(E_2)$.
2. $S(E_1) < S(E_2)$ whenever $\P(E_1) > \P(E_2)$.
3. If $E_1$ and $E_2$ are independent events then $S(E_1 \cap E_2) = S(E_1) + S(E_2)$.
</div>

In plain language: the first condition says that surprisal of an event depends only on its probability; the second condition says that low probability events are more surprising than high probability events; and the third condition says that surprise is additive when two independent events occur simultaneously.  Let us exhibit an example of a surprise function.

<div class="lemma">
Let $(\Omega, \Sigma, \P)$ be a measure space.  Then the function $S \colon \Sigma \to \R$ given by:

$$S(E) = -C \log \P(E)$$

is a surprise function for any positive constant $C$ (and any base for the logarithm).
</div>
<div class="proof">
Condition 1 in the definition of surprise function is obvious from the formula for $S$.  Condition 2 follows from the fact that the logarithm function is strictly increasing.  To prove condition 3, recall that $\P(E_1 \cap E_2) = \P(E_1)\P(E_2)$ whenever $E_1$ and $E_2$ are independent (by definition), so we have:

$$
\begin{align*}
S(E_1 \cap E_2) &= -C \log \P(E_1 \cap E_2) \\
&= -C \log \P(E_1) \P(E_2) \\
&= -C \log \P(E_1) - C \log \P(E_2) \\
&= S(E_1) + S(E_2)
\end{align*}
$$
</div>
