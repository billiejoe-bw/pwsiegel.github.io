---
layout: post
permalink: /ds/iaa/
title: Annotator Agreement in Machine Learning Experiments
abstract: 
level: low
date: 2018-02-12
categories: stage
comments: true
---

All of the pieces of your sandwich detection classifier are falling into place.
You collected 20000 images from Instagram, and by parsing hashtags you separated them into two classes: those that contain a sandwich and those that don't.
You trained a convolutional neural net to distinguish between the two classes, and the initial results are looking good.

But a lot is riding on the performance of your sandwich classifier, so you have to conduct an experiment to rigorously measure its performance.
You hire two annotators on Mechanical Turk to look through a sample of 1000 images and indicate whether or not each one contains a sandwich.
Let us denote the annotators by A and B, and let us use the numbers 0 and 1 to denote the labels "does not contain a sandwich" and "does contain a sandwich", respectively.
Here is a possible table of results from the annotation experiment:

| | A0 | A1 | Total |
|---|---|---|---|
| **B0** | 400 | 90 | 490 |
| **B1** | 60 | 450 | 510 |
| **Total** | 460 | 540 | 1000 |

In other words, there were 400 images which both annotators agreed contained no sandwich, 90 images which annotator A thought contained a sandwich but annotator B did not, and so on.

One immediate observation we can make using this table is that the two annotators agreed on the presence or absence of a sandwich in 850 out of 1000 of the images, or 85%.
We can view this as a measure of the quality of the problem and/or the experiment: if annotator agreement is low then it suggests that the annotation task is too subjective for statistical modelling to be effective.

Unfortunately, the raw rate of annotator agreement is not a suitable measure of the quality of an experiment, and we'll have to think a bit more carefully.

## Disagreeing with Agreement

To expose the subtleties involved, consider the extreme case where 99% of the images in the annotation set contain a sandwich.
Suppose that the annotators are lazy, and instead of carefully searching for sandwiches in each image they assign the label "has a sandwich" to 99% of of the images chosen at random and "doesn't have a sandwich" for the remaining 1%.
Then the rate of agreement will invariably be extremely high - they can't possibly disagree on more than 2% of the images - but the experiment could not possibly be useful since the annotators didn't even look at the data.

This issue is a symptom of a straightforward violation of basic statistical fundamentals.
The raw rate of agreement is the sum of the rate of "positive agreement" (both annotators agree that the image contains a sandwich) and "negative agreement" (both annotators agree that the image doesn't contain a sandwich).
Adding the two rates allows one to cover for the weakness of the other: if they are considered separately in the thought experiment above then positive agreement would be very high but negative agreement would be very low since the random annotators are unlikely to label the same 1% of the images as non-sandwich by accident.

In a moment I will propose some better methods for evaluating the results of annotation experiments, but first I will formulate some criteria that a good method should satisfy.

1. The method should generalize to more than two annotators and more than two categories.
2. The outputs of the method should be comparable across different experiments, regardless of the number of annotators, the number of categories, and the shape of the underlying data.
3. If an experiment is repeated and annotator agreement does not decrease in any category, then the overall evaluation of the second experiment must not be inferior to that of the first.

## The Kappa Konundrum

There is a standard way to correct the agreement rate in response to the objections above, first introduced by Scott in 1955.
The strategy is to compute a statistic of the form

$$\kappa = \frac{r - e}{1 - e}$$

where $r$ is the rate of agreement (the total number of agreements devided by the total number of annotations) and $e$ is the _expected agreement_.
There are a various different notions of what expected agreement should mean - see my post on the Kappa statistic for more detail - but roughly speaking $e$ represents the probability that the observed level of annotator agreement would occur through random chance.
So the numerator $r - e$ in the formula for $\kappa$ represents how much more (or less) agreement is observed compared to random annotators, and the denominator $1 - e$ is there to normalize the expression so that $-1 \leq \kappa \leq 1$.

This seems like a sensible way to address the concerns above, but numerous flaws have been raised in the literature.
The reader is again referred to my full post on $\kappa$ for further detail, but here is a summary:

- The error models used to define the expected agreement $e$ are indefensible in most practical experiments.
- The $\kappa$ statistic does generalize to arbitrarily many categories, but there are no non-arbitrary guidelines for comparing $\kappa$ scores across different experiments.
- Correcting for chance is not a necessary or standard practice in other evaluation paradigms, such as sensitivity / specificity for diagnostic tests or precision / recall for machine learning classifiers, and there is no particular reason it should be for annotator agreement.

In my brief literature review I was unable to find any serious responses to these objections, so my best guess is that $\kappa$ is a historical relic whose prevalence is explained by tradition rather than principles.
It is also possible that it makes more sense in disciplines like pyschology or medicine than in machine learning.

## Agreement by Category

The core problem with $\kappa$ scores is that they address a symptom rather than the disease.
Instead of concocting an elaborate scheme to correct the overall rate of annotator agreement, we should report the rate of agreement within each category.
In our example, this means reporting both the rate at which the annotators agree that a given image does contain a sandwich and the rate at which they agree that it does not.

This approach, advocated by Uebersax, has several advantages over the various $\kappa$ scores while still addressing the agreement-by-chance problem:

- The rates of agreement by category are easy to understand, and their interpretation doesn't depend on very many assumptions about annotator behavior.
- The method generalizes trivially to arbitrarily many categories, and in classification problems with many categories it is fairly typical that one would want to know how annotator agreement varies across categories.
- The rate of agreement for a category in one experiment is directly comparable to the rate of agreement for a category in another (though Uebersax recommends significance testing if a category is not highly represented in the annotation set).

The most serious disadvantage of this approach is that it involves reporting several numbers (one for each category) rather than summarizing the quality of the whole experiment with one metric.
If it is absolutely necessary to summarize the experiment with a single value, I recommend reporting the lowest rate of agreement: if this number is low then at least part of the annotation task was subjective, while if it is high (close to 1) then annotator agreement was high across the board.

## Calculations

Let us work though how to actually calculate the rate of annotator agreement for a given category.
We will do this for the general case in which any number annotators applies one or more of $m$ possible categories to $n$ samples.

Label the categories with a number $j \in \br{1, \ldots, m}$ and label the samples with a number $k \in \br{1, \ldots, n}$.
Let $c_{jk}$ denote the number of annotators who apply the category $j$ to the sample $k$, and let $c_k$ denote the total number of times the sample $k$ is annotated (i.e. the sum of the $c_{jk}$'s over all $j$).
In this computation we will allow $c_k$ to vary, meaning we do not assume that every annotator is exposed to every sample.

Following Uebersax, we will divide the total number of times a pair of annotators agreed that a sample has category $j$ by the number of potential agreements for $j$.
The number of agreements that sample $k$ has category $j$ is simply the number of pairs of annotators who assigned category $j$ to sample $k$.
This is the binomial coefficient $\binom{c_{jk}}{2}$.
It follows that the total number of agreements on the category $j$ is:

$$A_j = \sum_{k=1}^n \binom{c_{jk}}{2}$$

Counting potential agreements for the category $j$ is a little trickier.
Let us say that a pair of annotators _potentially_ agreed that sample $k$ has category $j$ if at least one of them assigned category $j$ to sample $k$.
The number of pairs of annotators for which both assigned category $j$ to sample $k$ is, again, $\binom{c_{jk}}{2}$, while the number of pairs for which exactly one member assigned category $j$ to sample $k$ is $c_{jk}(c_k - c_{jk})$.
So the total number of potential agreements for the category $j$ is:

$$P_j = \sum_{k=1}^n \binom{c_{jk}}{2} + c_{jk}(c_k - c_{jk})$$

Finally, the rate of agreement for the category $j$ is simply:

$$R_j = \frac{A_j}{P_j}$$

